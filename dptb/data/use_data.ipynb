{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from _build import dataset_from_config\n",
    "from dptb.utils.config import Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"root\": \"/root/nequip_data/\",\n",
    "    \"dataset\": \"npz\",\n",
    "    \"dataset_file_name\": \"/root/nequip_data/Si8-100K.npz\",\n",
    "    # \"include_keys\":[\n",
    "    #     \"kpoints\",\n",
    "    #     \"eigenvalues\"\n",
    "    # ],\n",
    "    \"key_mapping\":{\n",
    "        \"pos\":\"pos\",\n",
    "        \"atomic_numbers\":\"atomic_numbers\",\n",
    "        \"kpoints\": \"kpoint\",\n",
    "        \"pbc\": \"pbc\",\n",
    "        \"cell\": \"cell\",\n",
    "        \"eigenvalues\": \"eigenvalue\"\n",
    "    },\n",
    "    \"npz_fixed_field_keys\": [\"kpoint\", \"pbc\"],\n",
    "    \"graph_field\":[\"eigenvalues\"],\n",
    "    \"chemical_symbols\": [\"Si\", \"C\"],\n",
    "    \"r_max\": 4.0,\n",
    "    \"er_max\": 3.0\n",
    "}\n",
    "\n",
    "config = Config(config=config)\n",
    "# dataset: npz                                                                       # type of data set, can be npz or ase\n",
    "# dataset_url: http://quantum-machine.org/gdml/data/npz/toluene_ccsd_t.zip           # url to download the npz. optional\n",
    "# dataset_file_name: ./benchmark_data/toluene_ccsd_t-train.npz                       # path to data set file\n",
    "# key_mapping:\n",
    "#   z: atomic_numbers                                                                # atomic species, integers\n",
    "#   E: total_energy                                                                  # total potential eneriges to train to\n",
    "#   F: forces                                                                        # atomic forces to train to\n",
    "#   R: pos                                                                           # raw atomic positions\n",
    "# npz_fixed_field_keys:                                                              # fields that are repeated across different examples\n",
    "#   - atomic_numbers\n",
    "\n",
    "# chemical_symbols:\n",
    "#   - H\n",
    "#   - C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset_from_config(config=config, prefix=\"dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "from dataloader import DataLoader\n",
    "for i,x in enumerate(dataset):\n",
    "    dataset[i][\"edge_vectors\"] = x.get_edge_vectors()\n",
    "print(dataset[1][\"edge_vectors\"])\n",
    "loader = DataLoader(dataset, batch_size=3, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 354, 60])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from dptb.data.AtomicDataDict import with_edge_vectors\n",
    "for data in loader:\n",
    "    index = torch.arange(0,data.num_edges)[\n",
    "        (data.batch[data.edge_index[0]]==0) + (data.batch[data.edge_index[1]]==0)\n",
    "        ]\n",
    "    \n",
    "    print(data.eigenvalue.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0,  0,  0,  ..., 23, 23, 23],\n",
      "        [ 1,  6,  2,  ..., 20, 21, 22]])\n"
     ]
    }
   ],
   "source": [
    "from dptb.data.AtomicDataDict import with_edge_vectors\n",
    "for data in loader:\n",
    "    print(data[\"edge_index\"])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "npzdata = np.load(\"/root/nequip_data/toluene_ccsd_t-test.npz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['E', 'name', 'F', 'theory', 'R', 'z', 'type', 'md5']\n"
     ]
    }
   ],
   "source": [
    "print(npzdata.files)\n",
    "\n",
    "deeptb_data = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'd'\n"
     ]
    }
   ],
   "source": [
    "print(npzdata[\"type\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ase import Atoms\n",
    "from ase.io.trajectory import Trajectory\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "Traj = Trajectory(\"/data/band/IV/MD/100K/SiC/kpath.0/xdat.traj\")\n",
    "eigenvalues = np.load(\"/data/band/IV/MD/100K/SiC/kpath.0/eigs.npy\")\n",
    "kpoints = np.load(\"/data/band/IV/MD/100K/SiC/kpath.0/kpoints.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "cell = []\n",
    "pos = []\n",
    "atomic_numbers = []\n",
    "for i in Traj:\n",
    "    cell.append(i.cell.array)\n",
    "    pos.append(i.positions)\n",
    "    atomic_numbers.append(i.get_atomic_numbers())\n",
    "cell = np.array(cell)\n",
    "pos = np.array(pos)\n",
    "atomic_numbers = np.array(atomic_numbers)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = \"Si8-100K\"\n",
    "np.savez(\"/root/nequip_data/Si8-100K.npz\", cell=cell, pos=pos, atomic_numbers=atomic_numbers, kpoints=kpoints, eigenvalues=eigenvalues, pbc=[True, True, True])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "na = torch.nested.nested_tensor([torch.randn(n,n) for n in range(12)])\n",
    "\n",
    "nb = torch.nested.nested_tensor([torch.randn(n,n) for n in range(12)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "Could not run 'aten::permute' with arguments from the 'NestedTensorCPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::permute' is only available for these backends: [CPU, CUDA, HIP, XLA, MPS, IPU, XPU, HPU, VE, Lazy, Meta, PrivateUse1, PrivateUse2, PrivateUse3, FPGA, ORT, Vulkan, Metal, QuantizedCPU, QuantizedCUDA, QuantizedHIP, QuantizedXLA, QuantizedMPS, QuantizedIPU, QuantizedXPU, QuantizedHPU, QuantizedVE, QuantizedLazy, QuantizedMeta, QuantizedPrivateUse1, QuantizedPrivateUse2, QuantizedPrivateUse3, CustomRNGKeyId, MkldnnCPU, SparseCPU, SparseCUDA, SparseHIP, SparseXLA, SparseMPS, SparseIPU, SparseXPU, SparseHPU, SparseVE, SparseLazy, SparseMeta, SparsePrivateUse1, SparsePrivateUse2, SparsePrivateUse3, SparseCsrCPU, SparseCsrCUDA, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nUndefined: registered at aten/src/ATen/RegisterCompositeExplicitAutograd.cpp:6796 [default backend kernel]\nCPU: registered at aten/src/ATen/RegisterCompositeExplicitAutograd.cpp:6796 [default backend kernel]\nCUDA: registered at aten/src/ATen/RegisterCompositeExplicitAutograd.cpp:6796 [default backend kernel]\nHIP: registered at aten/src/ATen/RegisterCompositeExplicitAutograd.cpp:6796 [default backend kernel]\nXLA: registered at aten/src/ATen/RegisterCompositeExplicitAutograd.cpp:6796 [default backend kernel]\nMPS: registered at aten/src/ATen/RegisterCompositeExplicitAutograd.cpp:6796 [default backend kernel]\nIPU: registered at aten/src/ATen/RegisterCompositeExplicitAutograd.cpp:6796 [default backend kernel]\nXPU: registered at aten/src/ATen/RegisterCompositeExplicitAutograd.cpp:6796 [default backend kernel]\nHPU: registered at aten/src/ATen/RegisterCompositeExplicitAutograd.cpp:6796 [default backend kernel]\nVE: registered at aten/src/ATen/RegisterCompositeExplicitAutograd.cpp:6796 [default backend kernel]\nLazy: registered at aten/src/ATen/RegisterCompositeExplicitAutograd.cpp:6796 [default backend kernel]\nMeta: registered at aten/src/ATen/RegisterCompositeExplicitAutograd.cpp:6796 [default backend kernel]\nPrivateUse1: registered at aten/src/ATen/RegisterCompositeExplicitAutograd.cpp:6796 [default backend kernel]\nPrivateUse2: registered at aten/src/ATen/RegisterCompositeExplicitAutograd.cpp:6796 [default backend kernel]\nPrivateUse3: registered at aten/src/ATen/RegisterCompositeExplicitAutograd.cpp:6796 [default backend kernel]\nFPGA: registered at aten/src/ATen/RegisterCompositeExplicitAutograd.cpp:6796 [default backend kernel]\nORT: registered at aten/src/ATen/RegisterCompositeExplicitAutograd.cpp:6796 [default backend kernel]\nVulkan: registered at aten/src/ATen/RegisterCompositeExplicitAutograd.cpp:6796 [default backend kernel]\nMetal: registered at aten/src/ATen/RegisterCompositeExplicitAutograd.cpp:6796 [default backend kernel]\nQuantizedCPU: registered at aten/src/ATen/RegisterCompositeExplicitAutograd.cpp:6796 [default backend kernel]\nQuantizedCUDA: registered at aten/src/ATen/RegisterCompositeExplicitAutograd.cpp:6796 [default backend kernel]\nQuantizedHIP: registered at aten/src/ATen/RegisterCompositeExplicitAutograd.cpp:6796 [default backend kernel]\nQuantizedXLA: registered at aten/src/ATen/RegisterCompositeExplicitAutograd.cpp:6796 [default backend kernel]\nQuantizedMPS: registered at aten/src/ATen/RegisterCompositeExplicitAutograd.cpp:6796 [default backend kernel]\nQuantizedIPU: registered at aten/src/ATen/RegisterCompositeExplicitAutograd.cpp:6796 [default backend kernel]\nQuantizedXPU: registered at aten/src/ATen/RegisterCompositeExplicitAutograd.cpp:6796 [default backend kernel]\nQuantizedHPU: registered at aten/src/ATen/RegisterCompositeExplicitAutograd.cpp:6796 [default backend kernel]\nQuantizedVE: registered at aten/src/ATen/RegisterCompositeExplicitAutograd.cpp:6796 [default backend kernel]\nQuantizedLazy: registered at aten/src/ATen/RegisterCompositeExplicitAutograd.cpp:6796 [default backend kernel]\nQuantizedMeta: registered at aten/src/ATen/RegisterCompositeExplicitAutograd.cpp:6796 [default backend kernel]\nQuantizedPrivateUse1: registered at aten/src/ATen/RegisterCompositeExplicitAutograd.cpp:6796 [default backend kernel]\nQuantizedPrivateUse2: registered at aten/src/ATen/RegisterCompositeExplicitAutograd.cpp:6796 [default backend kernel]\nQuantizedPrivateUse3: registered at aten/src/ATen/RegisterCompositeExplicitAutograd.cpp:6796 [default backend kernel]\nCustomRNGKeyId: registered at aten/src/ATen/RegisterCompositeExplicitAutograd.cpp:6796 [default backend kernel]\nMkldnnCPU: registered at aten/src/ATen/RegisterCompositeExplicitAutograd.cpp:6796 [default backend kernel]\nSparseCPU: registered at aten/src/ATen/RegisterSparseCPU.cpp:1261 [kernel]\nSparseCUDA: registered at aten/src/ATen/RegisterCompositeExplicitAutograd.cpp:6796 [default backend kernel]\nSparseHIP: registered at aten/src/ATen/RegisterCompositeExplicitAutograd.cpp:6796 [default backend kernel]\nSparseXLA: registered at aten/src/ATen/RegisterCompositeExplicitAutograd.cpp:6796 [default backend kernel]\nSparseMPS: registered at aten/src/ATen/RegisterCompositeExplicitAutograd.cpp:6796 [default backend kernel]\nSparseIPU: registered at aten/src/ATen/RegisterCompositeExplicitAutograd.cpp:6796 [default backend kernel]\nSparseXPU: registered at aten/src/ATen/RegisterCompositeExplicitAutograd.cpp:6796 [default backend kernel]\nSparseHPU: registered at aten/src/ATen/RegisterCompositeExplicitAutograd.cpp:6796 [default backend kernel]\nSparseVE: registered at aten/src/ATen/RegisterCompositeExplicitAutograd.cpp:6796 [default backend kernel]\nSparseLazy: registered at aten/src/ATen/RegisterCompositeExplicitAutograd.cpp:6796 [default backend kernel]\nSparseMeta: registered at aten/src/ATen/RegisterCompositeExplicitAutograd.cpp:6796 [default backend kernel]\nSparsePrivateUse1: registered at aten/src/ATen/RegisterCompositeExplicitAutograd.cpp:6796 [default backend kernel]\nSparsePrivateUse2: registered at aten/src/ATen/RegisterCompositeExplicitAutograd.cpp:6796 [default backend kernel]\nSparsePrivateUse3: registered at aten/src/ATen/RegisterCompositeExplicitAutograd.cpp:6796 [default backend kernel]\nSparseCsrCPU: registered at aten/src/ATen/RegisterCompositeExplicitAutograd.cpp:6796 [default backend kernel]\nSparseCsrCUDA: registered at aten/src/ATen/RegisterCompositeExplicitAutograd.cpp:6796 [default backend kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at ../aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at aten/src/ATen/RegisterFunctionalization_0.cpp:19962 [kernel]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: fallthrough registered at ../aten/src/ATen/ConjugateFallback.cpp:22 [kernel]\nNegative: fallthrough registered at ../aten/src/ATen/native/NegateFallback.cpp:22 [kernel]\nZeroTensor: fallthrough registered at ../aten/src/ATen/ZeroTensorFallback.cpp:90 [kernel]\nADInplaceOrView: registered at ../torch/csrc/autograd/generated/ADInplaceOrViewType_0.cpp:4822 [kernel]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:15931 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:15931 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:15931 [autograd kernel]\nAutogradHIP: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:15931 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:15931 [autograd kernel]\nAutogradMPS: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:15931 [autograd kernel]\nAutogradIPU: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:15931 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:15931 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:15931 [autograd kernel]\nAutogradVE: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:15931 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:15931 [autograd kernel]\nAutogradMeta: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:15931 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:15931 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:15931 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:15931 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:15931 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_3.cpp:14484 [kernel]\nAutocastCPU: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastCUDA: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at ../aten/src/ATen/functorch/BatchRulesViews.cpp:512 [kernel]\nFuncTorchVmapMode: fallthrough registered at ../aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at ../aten/src/ATen/BatchingRegistrations.cpp:1068 [kernel]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at ../aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at ../aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m/root/deeptb/dptb/data/use_data.ipynb Cell 16\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bdeeptb_dev2/root/deeptb/dptb/data/use_data.ipynb#X24sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m na\u001b[39m.\u001b[39;49mpermute(\u001b[39m1\u001b[39;49m,\u001b[39m2\u001b[39;49m,\u001b[39m0\u001b[39;49m)\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: Could not run 'aten::permute' with arguments from the 'NestedTensorCPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::permute' is only available for these backends: [CPU, CUDA, HIP, XLA, MPS, IPU, XPU, HPU, VE, Lazy, Meta, PrivateUse1, PrivateUse2, PrivateUse3, FPGA, ORT, Vulkan, Metal, QuantizedCPU, QuantizedCUDA, QuantizedHIP, QuantizedXLA, QuantizedMPS, QuantizedIPU, QuantizedXPU, QuantizedHPU, QuantizedVE, QuantizedLazy, QuantizedMeta, QuantizedPrivateUse1, QuantizedPrivateUse2, QuantizedPrivateUse3, CustomRNGKeyId, MkldnnCPU, SparseCPU, SparseCUDA, SparseHIP, SparseXLA, SparseMPS, SparseIPU, SparseXPU, SparseHPU, SparseVE, SparseLazy, SparseMeta, SparsePrivateUse1, SparsePrivateUse2, SparsePrivateUse3, SparseCsrCPU, SparseCsrCUDA, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nUndefined: registered at aten/src/ATen/RegisterCompositeExplicitAutograd.cpp:6796 [default backend kernel]\nCPU: registered at aten/src/ATen/RegisterCompositeExplicitAutograd.cpp:6796 [default backend kernel]\nCUDA: registered at aten/src/ATen/RegisterCompositeExplicitAutograd.cpp:6796 [default backend kernel]\nHIP: registered at aten/src/ATen/RegisterCompositeExplicitAutograd.cpp:6796 [default backend kernel]\nXLA: registered at aten/src/ATen/RegisterCompositeExplicitAutograd.cpp:6796 [default backend kernel]\nMPS: registered at aten/src/ATen/RegisterCompositeExplicitAutograd.cpp:6796 [default backend kernel]\nIPU: registered at aten/src/ATen/RegisterCompositeExplicitAutograd.cpp:6796 [default backend kernel]\nXPU: registered at aten/src/ATen/RegisterCompositeExplicitAutograd.cpp:6796 [default backend kernel]\nHPU: registered at aten/src/ATen/RegisterCompositeExplicitAutograd.cpp:6796 [default backend kernel]\nVE: registered at aten/src/ATen/RegisterCompositeExplicitAutograd.cpp:6796 [default backend kernel]\nLazy: registered at aten/src/ATen/RegisterCompositeExplicitAutograd.cpp:6796 [default backend kernel]\nMeta: registered at aten/src/ATen/RegisterCompositeExplicitAutograd.cpp:6796 [default backend kernel]\nPrivateUse1: registered at aten/src/ATen/RegisterCompositeExplicitAutograd.cpp:6796 [default backend kernel]\nPrivateUse2: registered at aten/src/ATen/RegisterCompositeExplicitAutograd.cpp:6796 [default backend kernel]\nPrivateUse3: registered at aten/src/ATen/RegisterCompositeExplicitAutograd.cpp:6796 [default backend kernel]\nFPGA: registered at aten/src/ATen/RegisterCompositeExplicitAutograd.cpp:6796 [default backend kernel]\nORT: registered at aten/src/ATen/RegisterCompositeExplicitAutograd.cpp:6796 [default backend kernel]\nVulkan: registered at aten/src/ATen/RegisterCompositeExplicitAutograd.cpp:6796 [default backend kernel]\nMetal: registered at aten/src/ATen/RegisterCompositeExplicitAutograd.cpp:6796 [default backend kernel]\nQuantizedCPU: registered at aten/src/ATen/RegisterCompositeExplicitAutograd.cpp:6796 [default backend kernel]\nQuantizedCUDA: registered at aten/src/ATen/RegisterCompositeExplicitAutograd.cpp:6796 [default backend kernel]\nQuantizedHIP: registered at aten/src/ATen/RegisterCompositeExplicitAutograd.cpp:6796 [default backend kernel]\nQuantizedXLA: registered at aten/src/ATen/RegisterCompositeExplicitAutograd.cpp:6796 [default backend kernel]\nQuantizedMPS: registered at aten/src/ATen/RegisterCompositeExplicitAutograd.cpp:6796 [default backend kernel]\nQuantizedIPU: registered at aten/src/ATen/RegisterCompositeExplicitAutograd.cpp:6796 [default backend kernel]\nQuantizedXPU: registered at aten/src/ATen/RegisterCompositeExplicitAutograd.cpp:6796 [default backend kernel]\nQuantizedHPU: registered at aten/src/ATen/RegisterCompositeExplicitAutograd.cpp:6796 [default backend kernel]\nQuantizedVE: registered at aten/src/ATen/RegisterCompositeExplicitAutograd.cpp:6796 [default backend kernel]\nQuantizedLazy: registered at aten/src/ATen/RegisterCompositeExplicitAutograd.cpp:6796 [default backend kernel]\nQuantizedMeta: registered at aten/src/ATen/RegisterCompositeExplicitAutograd.cpp:6796 [default backend kernel]\nQuantizedPrivateUse1: registered at aten/src/ATen/RegisterCompositeExplicitAutograd.cpp:6796 [default backend kernel]\nQuantizedPrivateUse2: registered at aten/src/ATen/RegisterCompositeExplicitAutograd.cpp:6796 [default backend kernel]\nQuantizedPrivateUse3: registered at aten/src/ATen/RegisterCompositeExplicitAutograd.cpp:6796 [default backend kernel]\nCustomRNGKeyId: registered at aten/src/ATen/RegisterCompositeExplicitAutograd.cpp:6796 [default backend kernel]\nMkldnnCPU: registered at aten/src/ATen/RegisterCompositeExplicitAutograd.cpp:6796 [default backend kernel]\nSparseCPU: registered at aten/src/ATen/RegisterSparseCPU.cpp:1261 [kernel]\nSparseCUDA: registered at aten/src/ATen/RegisterCompositeExplicitAutograd.cpp:6796 [default backend kernel]\nSparseHIP: registered at aten/src/ATen/RegisterCompositeExplicitAutograd.cpp:6796 [default backend kernel]\nSparseXLA: registered at aten/src/ATen/RegisterCompositeExplicitAutograd.cpp:6796 [default backend kernel]\nSparseMPS: registered at aten/src/ATen/RegisterCompositeExplicitAutograd.cpp:6796 [default backend kernel]\nSparseIPU: registered at aten/src/ATen/RegisterCompositeExplicitAutograd.cpp:6796 [default backend kernel]\nSparseXPU: registered at aten/src/ATen/RegisterCompositeExplicitAutograd.cpp:6796 [default backend kernel]\nSparseHPU: registered at aten/src/ATen/RegisterCompositeExplicitAutograd.cpp:6796 [default backend kernel]\nSparseVE: registered at aten/src/ATen/RegisterCompositeExplicitAutograd.cpp:6796 [default backend kernel]\nSparseLazy: registered at aten/src/ATen/RegisterCompositeExplicitAutograd.cpp:6796 [default backend kernel]\nSparseMeta: registered at aten/src/ATen/RegisterCompositeExplicitAutograd.cpp:6796 [default backend kernel]\nSparsePrivateUse1: registered at aten/src/ATen/RegisterCompositeExplicitAutograd.cpp:6796 [default backend kernel]\nSparsePrivateUse2: registered at aten/src/ATen/RegisterCompositeExplicitAutograd.cpp:6796 [default backend kernel]\nSparsePrivateUse3: registered at aten/src/ATen/RegisterCompositeExplicitAutograd.cpp:6796 [default backend kernel]\nSparseCsrCPU: registered at aten/src/ATen/RegisterCompositeExplicitAutograd.cpp:6796 [default backend kernel]\nSparseCsrCUDA: registered at aten/src/ATen/RegisterCompositeExplicitAutograd.cpp:6796 [default backend kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at ../aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at aten/src/ATen/RegisterFunctionalization_0.cpp:19962 [kernel]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: fallthrough registered at ../aten/src/ATen/ConjugateFallback.cpp:22 [kernel]\nNegative: fallthrough registered at ../aten/src/ATen/native/NegateFallback.cpp:22 [kernel]\nZeroTensor: fallthrough registered at ../aten/src/ATen/ZeroTensorFallback.cpp:90 [kernel]\nADInplaceOrView: registered at ../torch/csrc/autograd/generated/ADInplaceOrViewType_0.cpp:4822 [kernel]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:15931 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:15931 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:15931 [autograd kernel]\nAutogradHIP: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:15931 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:15931 [autograd kernel]\nAutogradMPS: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:15931 [autograd kernel]\nAutogradIPU: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:15931 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:15931 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:15931 [autograd kernel]\nAutogradVE: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:15931 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:15931 [autograd kernel]\nAutogradMeta: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:15931 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:15931 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:15931 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:15931 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:15931 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_3.cpp:14484 [kernel]\nAutocastCPU: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastCUDA: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at ../aten/src/ATen/functorch/BatchRulesViews.cpp:512 [kernel]\nFuncTorchVmapMode: fallthrough registered at ../aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at ../aten/src/ATen/BatchingRegistrations.cpp:1068 [kernel]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at ../aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at ../aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1254711151123047\n"
     ]
    }
   ],
   "source": [
    "out = []\n",
    "start = time.time()\n",
    "for n in range(12000):\n",
    "    m = torch.randn(3,3)\n",
    "    out.append(m @ m)\n",
    "\n",
    "end = time.time()\n",
    "print(end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "mp = torch.bmm(na, na)\n",
    "print(mp[0] - a @ a)\n",
    "print(mp[1] - b @ b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True False\n"
     ]
    }
   ],
   "source": [
    "print(mp.is_nested, a.is_nested)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeptb",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

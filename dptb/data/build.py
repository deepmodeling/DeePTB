import inspect
from importlib import import_module
from dptb.data.dataset import ABACUSDataset, ABACUSInMemoryDataset, DefaultDataset
from dptb import data
from dptb.data.transforms import TypeMapper, OrbitalMapper
from dptb.data import AtomicDataset, register_fields
from dptb.utils import instantiate, get_w_prefix


def dataset_from_config(config, prefix: str = "dataset") -> AtomicDataset:
    """initialize database based on a config instance

    It needs dataset type name (case insensitive),
    and all the parameters needed in the constructor.

    Examples see tests/data/test_dataset.py TestFromConfig
    and tests/datasets/test_simplest.py

    Args:

    config (dict, nequip.utils.Config): dict/object that store all the parameters
    prefix (str): Optional. The prefix of all dataset parameters

    Return:

    dataset (nequip.data.AtomicDataset)
    """

    config_dataset = config.get(prefix, None)
    if config_dataset is None:
        raise KeyError(f"Dataset with prefix `{prefix}` isn't present in this config!")

    if inspect.isclass(config_dataset):
        # user define class
        class_name = config_dataset
    else:
        try:
            module_name = ".".join(config_dataset.split(".")[:-1])
            class_name = ".".join(config_dataset.split(".")[-1:])
            class_name = getattr(import_module(module_name), class_name)
        except Exception:
            # ^ TODO: don't catch all Exception
            # default class defined in nequip.data or nequip.dataset
            dataset_name = config_dataset.lower()

            class_name = None
            for k, v in inspect.getmembers(data, inspect.isclass):
                if k.endswith("Dataset"):
                    if k.lower() == dataset_name:
                        class_name = v
                    if k[:-7].lower() == dataset_name:
                        class_name = v
                elif k.lower() == dataset_name:
                    class_name = v

    if class_name is None:
        raise NameError(f"dataset type {dataset_name} does not exists")

    # if dataset r_max is not found, use the universal r_max
    atomicdata_options_key = "AtomicData_options"
    prefixed_eff_key = f"{prefix}_{atomicdata_options_key}"
    config[prefixed_eff_key] = get_w_prefix(
        atomicdata_options_key, {}, prefix=prefix, arg_dicts=config
    )
    config[prefixed_eff_key]["r_max"] = get_w_prefix(
        "r_max",
        prefix=prefix,
        arg_dicts=[config[prefixed_eff_key], config],
    )

    config[prefixed_eff_key]["er_max"] = get_w_prefix(
        "er_max",
        prefix=prefix,
        arg_dicts=[config[prefixed_eff_key], config],
    )

    config[prefixed_eff_key]["oer_max"] = get_w_prefix(
        "oer_max",
        prefix=prefix,
        arg_dicts=[config[prefixed_eff_key], config],
    )

    # Build a TypeMapper from the config
    type_mapper, _ = instantiate(TypeMapper, prefix=prefix, optional_args=config)

    # Register fields:
    # This might reregister fields, but that's OK:
    instantiate(register_fields, all_args=config)

    instance, _ = instantiate(
        class_name,
        prefix=prefix,
        positional_args={"type_mapper": type_mapper},
        optional_args=config,
    )

    return instance


def build_dataset(set_options, common_options):

    AtomicDataOptions = {
        "r_max": common_options["bond_cutoff"],
        "er_max": common_options.get("env_cutoff", None),
        "oer_max": common_options.get("onsite_cutoff", None),
    }

    AtomicDataOptions.update(set_options.get("AtomicData_options", {}))

    type = set_options["type"]

    # input in set_option needed for ABACUS Dataset:
    # "root": `.pth` file is saved in root, NO data read from here.
    # "preprocess_dir": the same of "preprocess_dir" assigned in `dptb data`, 
    #                   contains all necessary data files generated by `dptb data`.
    # "pbc": must be specifiy here, true / false.
    # "included_frames": optional list, for loading InMemory version.
    # Example:
    # "train": {
    #        "type": "ABACUSInMemoryDataset",
    #        "root": "no/AtomicData/files/here",
    #        "preprocess_dir": "same/as/in/dptb_data/input_json",
    #        "pbc": true,
    #        "included_frames": [1,2,3]
    #    }
    if type == "ABACUSDataset":
        assert "pbc" in set_options, "PBC must be provided in `data_options` when loading ABACUS dataset."
        AtomicDataOptions["pbc"] = set_options["pbc"]
        if "basis" in common_options:
            idp = OrbitalMapper(common_options["basis"])
        else:
            idp = None
        dataset = ABACUSDataset(
            root=set_options["root"],
            preprocess_dir=set_options["preprocess_dir"],
            AtomicData_options=AtomicDataOptions,
            type_mapper=idp,
        )
    elif type == "ABACUSInMemoryDataset":
        assert "pbc" in set_options, "PBC must be provided in `data_options` when loading ABACUS dataset."
        AtomicDataOptions["pbc"] = set_options["pbc"]
        if "basis" in common_options:
            idp = OrbitalMapper(common_options["basis"])
        else:
            idp = None
        dataset = ABACUSInMemoryDataset(
            root=set_options["root"],
            preprocess_dir=set_options["preprocess_dir"],
            include_frames=set_options.get("include_frames"),
            AtomicData_options=AtomicDataOptions,
            type_mapper=idp,
        )     
    
    # input in common_option for Default Dataset:
    # "lcao_basis": optional, dict like {"C": "2s2p1d"}. 
    #               Must be provided when loading Hamiltonian.
    # input in set_option for Default Dataset:
    # "root": main dir storing all trajectory folders.
    # "prefix": optional, load selected trajectory folders.
    # Example:
    # "train": {
    #        "type": "DefaultDataset",
    #        "root": "foo/bar/data_files_nere",
    #        "prefix": "traj"
    #    }
    elif type == "DefaultDataset":
        if "basis" in common_options:
            idp = OrbitalMapper(common_options["basis"])
        else:
            idp = None
        dataset = DefaultDataset(
            root=set_options["root"],
            AtomicData_options=AtomicDataOptions,
            type_mapper=idp,
            prefix=set_options.get("prefix", None)
        )

    else:
        raise ValueError(f"Not support dataset type: {type}.")

    return dataset
